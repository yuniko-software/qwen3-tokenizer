using Yuniko.Software.Qwen3Tokenizer;

Console.WriteLine("=== Qwen3 Tokenizer Sample ===\n");

// ============================================================================
// SECTION 1: Loading the Tokenizer
// ============================================================================
Console.WriteLine("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
Console.WriteLine("â•‘ SECTION 1: Loading the Tokenizer                             â•‘");
Console.WriteLine("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

// Method 1: Load from HuggingFace (downloads and caches files automatically)
Console.WriteLine("â†’ Method 1: FromHuggingFace (sync)");
var tokenizer = Qwen3Tokenizer.FromHuggingFace(
    modelName: "Qwen/Qwen3-Embedding-0.6B"
);
Console.WriteLine($"  âœ“ Tokenizer loaded! Vocabulary size: {tokenizer.VocabularySize:N0}\n");

// Method 2: Load from HuggingFace with progress reporting (async)
Console.WriteLine("â†’ Method 2: FromHuggingFaceAsync with progress");
var progress = new Progress<DownloadProgress>(p =>
{
    if (p.PercentComplete.HasValue)
    {
        Console.WriteLine($"  [{p.FileName}] {p.PercentComplete:F1}% - {p.Status}");
    }
    else
    {
        Console.WriteLine($"  [{p.FileName}] {p.Status}");
    }
});

var tokenizerAsync = await Qwen3Tokenizer.FromHuggingFaceAsync(
    modelName: "Qwen/Qwen3-Embedding-0.6B",
    progress: progress
);
Console.WriteLine($"  âœ“ Async tokenizer loaded!\n");

// Method 3: Load from local files (if you already have the files)
Console.WriteLine("â†’ Method 3: FromFiles (direct path loading)");
Console.WriteLine("  Example: Qwen3Tokenizer.FromFiles(\"path/to/vocab.json\", \"path/to/merges.txt\")");
Console.WriteLine("  (Skipped - requires local files)\n");

// Method 4: Load with custom configuration
Console.WriteLine("â†’ Method 4: FromHuggingFace with custom config");
var config = new HuggingFaceConfig(
    BaseUrl: "https://huggingface.co",
    Branch: "main",
    VocabFileName: "vocab.json",
    MergesFileName: "merges.txt"
);
var provider = new HuggingFaceFileProvider("Qwen/Qwen3-Embedding-0.6B", config: config);
var tokenizerCustom = Qwen3Tokenizer.FromProvider(provider);
Console.WriteLine($"  âœ“ Custom tokenizer loaded!\n");

Console.WriteLine("Press any key to continue...\n");
Console.ReadKey();

// ============================================================================
// SECTION 2: Basic Tokenization
// ============================================================================
Console.WriteLine("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
Console.WriteLine("â•‘ SECTION 2: Basic Tokenization                                â•‘");
Console.WriteLine("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

string sampleText = "Hello, this is a test of the Qwen3 tokenizer!";

// Encode text to token IDs
Console.WriteLine("â†’ Encode (text â†’ token IDs)");
Console.WriteLine($"  Input: \"{sampleText}\"");
int[] tokenIds = tokenizer.Encode(sampleText);
Console.WriteLine($"  Output: [{string.Join(", ", tokenIds)}]");
Console.WriteLine($"  Token count: {tokenIds.Length}\n");

// Decode token IDs back to text
Console.WriteLine("â†’ Decode (token IDs â†’ text)");
Console.WriteLine($"  Input: [{string.Join(", ", tokenIds)}]");
string decodedText = tokenizer.Decode(tokenIds);
Console.WriteLine($"  Output: \"{decodedText}\"\n");

// Count tokens without full encoding (more efficient)
Console.WriteLine("â†’ CountTokens (efficient token counting)");
string longText = "The Qwen3 tokenizer is designed for Qwen3-Embedding models.";
int count = tokenizer.CountTokens(longText);
Console.WriteLine($"  Input: \"{longText}\"");
Console.WriteLine($"  Token count: {count}\n");

Console.WriteLine("Press any key to continue...\n");
Console.ReadKey();

// ============================================================================
// SECTION 3: Advanced Tokenization
// ============================================================================
Console.WriteLine("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
Console.WriteLine("â•‘ SECTION 3: Advanced Tokenization                             â•‘");
Console.WriteLine("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

// Detailed encoding with token information
Console.WriteLine("â†’ EncodeDetailed (get tokens, IDs, and offsets)");
string detailText = "Tokenization is fun!";
var detailed = tokenizer.EncodeDetailed(detailText);
Console.WriteLine($"  Input: \"{detailText}\"\n");
Console.WriteLine("  Token Details:");
Console.WriteLine("  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
Console.WriteLine("  â”‚ Idx â”‚ ID     â”‚ Token                â”‚ Offset       â”‚");
Console.WriteLine("  â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
for (int i = 0; i < detailed.Tokens.Length; i++)
{
    Console.WriteLine($"  â”‚ {i,3} â”‚ {detailed.Ids[i],6} â”‚ {detailed.Tokens[i],-20} â”‚ ({detailed.Offsets[i].Index,3}, {detailed.Offsets[i].Length,2})     â”‚");
}
Console.WriteLine("  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n");

// Batch encoding
Console.WriteLine("â†’ EncodeBatch (encode multiple texts at once)");
string[] batchTexts = ["First sentence.", "Second sentence.", "Third sentence."];
int[][] batchIds = tokenizer.EncodeBatch(batchTexts);
Console.WriteLine("  Inputs:");
for (int i = 0; i < batchTexts.Length; i++)
{
    Console.WriteLine($"    {i + 1}. \"{batchTexts[i]}\"");
}
Console.WriteLine("\n  Outputs:");
for (int i = 0; i < batchIds.Length; i++)
{
    Console.WriteLine($"    {i + 1}. [{string.Join(", ", batchIds[i])}]");
}
Console.WriteLine();

// Batch decoding
Console.WriteLine("â†’ DecodeBatch (decode multiple token sequences at once)");
string[] decodedBatch = tokenizer.DecodeBatch(batchIds);
Console.WriteLine("  Decoded:");
for (int i = 0; i < decodedBatch.Length; i++)
{
    Console.WriteLine($"    {i + 1}. \"{decodedBatch[i]}\"");
}
Console.WriteLine();

Console.WriteLine("Press any key to continue...\n");
Console.ReadKey();

// ============================================================================
// SECTION 4: Working with Special Tokens
// ============================================================================
Console.WriteLine("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
Console.WriteLine("â•‘ SECTION 4: Working with Special Tokens                       â•‘");
Console.WriteLine("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

// List all special tokens
Console.WriteLine("â†’ Available Special Tokens");
Console.WriteLine("  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”");
Console.WriteLine("  â”‚ Token                     â”‚ ID     â”‚");
Console.WriteLine("  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
foreach (var (token, id) in tokenizer.SpecialTokens)
{
    Console.WriteLine($"  â”‚ {token,-25} â”‚ {id,6} â”‚");
}
Console.WriteLine("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n");

// Encode with special tokens
Console.WriteLine("â†’ Encoding with Special Tokens");
string textWithSpecial = "<|im_start|>user\nHello!<|im_end|>";
Console.WriteLine($"  Input: \"{textWithSpecial}\"");
var idsWithSpecial = tokenizer.Encode(textWithSpecial, addEos: false);
Console.WriteLine($"  Encoded: [{string.Join(", ", idsWithSpecial)}]\n");

// Decode with and without special tokens
Console.WriteLine("â†’ Decoding with/without Special Tokens");
string withSpecialTokens = tokenizer.Decode(idsWithSpecial, skipSpecialTokens: false);
string withoutSpecialTokens = tokenizer.Decode(idsWithSpecial, skipSpecialTokens: true);
Console.WriteLine($"  With special tokens:    \"{withSpecialTokens}\"");
Console.WriteLine($"  Without special tokens: \"{withoutSpecialTokens}\"\n");

// Get specific special token IDs
Console.WriteLine("â†’ Get Specific Special Token ID");
int? eosId = tokenizer.GetSpecialTokenId("<|endoftext|>");
Console.WriteLine($"  GetSpecialTokenId(\"<|endoftext|>\"): {eosId}\n");

Console.WriteLine("Press any key to continue...\n");
Console.ReadKey();

// ============================================================================
// SECTION 5: ONNX Runtime Integration
// ============================================================================
Console.WriteLine("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
Console.WriteLine("â•‘ SECTION 5: ONNX Runtime Integration                          â•‘");
Console.WriteLine("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

// Prepare single text for ONNX
Console.WriteLine("â†’ PrepareForOnnx (single text)");
string onnxText = "Example text for ONNX inference";
var (inputIds, attentionMask) = tokenizer.PrepareForOnnx(onnxText, maxLength: 20);
Console.WriteLine($"  Input: \"{onnxText}\"");
Console.WriteLine($"  Max Length: 20\n");
Console.WriteLine($"  Input IDs:      [{string.Join(", ", inputIds)}]");
Console.WriteLine($"  Attention Mask: [{string.Join(", ", attentionMask)}]\n");

// Prepare batch for ONNX
Console.WriteLine("â†’ PrepareForOnnxBatch (batch of texts)");
string[] onnxBatch = ["First text.", "Second text with more tokens."];
var (batchInputIds, batchAttentionMask) = tokenizer.PrepareForOnnxBatch(onnxBatch, maxLength: 15);
Console.WriteLine($"  Batch size: {onnxBatch.Length}");
Console.WriteLine($"  Max Length: 15\n");
for (int i = 0; i < onnxBatch.Length; i++)
{
    Console.WriteLine($"  Text {i + 1}: \"{onnxBatch[i]}\"");
    Console.Write($"    Input IDs:      [");
    for (int j = 0; j < 15; j++)
    {
        Console.Write($"{batchInputIds[i, j]}{(j < 14 ? ", " : "")}");
    }

    Console.WriteLine("]");
    Console.Write($"    Attention Mask: [");
    for (int j = 0; j < 15; j++)
    {
        Console.Write($"{batchAttentionMask[i, j]}{(j < 14 ? ", " : "")}");
    }

    Console.WriteLine("]\n");
}

Console.WriteLine("Press any key to continue...\n");
Console.ReadKey();

// ============================================================================
// SECTION 6: Custom Options and Configuration
// ============================================================================
Console.WriteLine("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
Console.WriteLine("â•‘ SECTION 6: Custom Options and Configuration                  â•‘");
Console.WriteLine("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

// Using default options
Console.WriteLine("â†’ Default Tokenizer Options");
var defaultOptions = Qwen3TokenizerOptions.Default;
Console.WriteLine($"  EOS Token ID: {defaultOptions.EosTokenId}");
Console.WriteLine($"  Byte Level: {defaultOptions.ByteLevel}");
Console.WriteLine($"  Special Tokens Count: {defaultOptions.SpecialTokens.Count}\n");

// Creating custom options
Console.WriteLine("â†’ Custom Tokenizer Options (using record 'with' expression)");
var customOptionsExample = Qwen3TokenizerOptions.Default with
{
    ByteLevel = false
};
Console.WriteLine($"  Custom ByteLevel: {customOptionsExample.ByteLevel}");
Console.WriteLine("  Usage: Qwen3Tokenizer.FromHuggingFace(..., options: customOptions)\n");

// Custom HuggingFace configuration
Console.WriteLine("â†’ Custom HuggingFace Configuration");
var customConfig = new HuggingFaceConfig(
    BaseUrl: "https://huggingface.co",
    Branch: "v1.0.0",  // Use specific version/tag
    VocabFileName: "vocab.json",
    MergesFileName: "merges.txt"
);
Console.WriteLine($"  Base URL: {customConfig.BaseUrl}");
Console.WriteLine($"  Branch: {customConfig.Branch}");
Console.WriteLine($"  Vocab File: {customConfig.VocabFileName}");
Console.WriteLine($"  Merges File: {customConfig.MergesFileName}\n");

// Using custom HttpClient
Console.WriteLine("â†’ Custom HttpClient (for authentication, proxies, etc.)");
Console.WriteLine("  Example:");
Console.WriteLine("    using var httpClient = new HttpClient();");
Console.WriteLine("    httpClient.DefaultRequestHeaders.Add(\"Authorization\", \"Bearer YOUR_TOKEN\");");
Console.WriteLine("    var tokenizer = Qwen3Tokenizer.FromHuggingFace(");
Console.WriteLine("        modelName: \"...\",");
Console.WriteLine("        httpClient: httpClient");
Console.WriteLine("    );\n");

// Special token constants
Console.WriteLine("â†’ Special Token Constants");
Console.WriteLine($"  EndOfText Token ID: {Qwen3EmbeddingModelSpecialTokens.EndOfTextTokenId}");
Console.WriteLine($"  ImStart Token ID: {Qwen3EmbeddingModelSpecialTokens.ImStartTokenId}");
Console.WriteLine($"  ImEnd Token ID: {Qwen3EmbeddingModelSpecialTokens.ImEndTokenId}");
Console.WriteLine($"  EndOfText String: \"{Qwen3EmbeddingModelSpecialTokens.EndOfText}\"");
Console.WriteLine($"  ImStart String: \"{Qwen3EmbeddingModelSpecialTokens.ImStart}\"\n");

// ============================================================================
// SECTION 7: Summary of API Methods
// ============================================================================
Console.WriteLine("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
Console.WriteLine("â•‘ SECTION 7: Summary of API Methods                            â•‘");
Console.WriteLine("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

Console.WriteLine("ğŸ“¦ LOADING:");
Console.WriteLine("  â€¢ FromFiles(vocabPath, mergesPath, options?)");
Console.WriteLine("  â€¢ FromHuggingFace(modelName, cacheDir?, options?, httpClient?, progress?)");
Console.WriteLine("  â€¢ FromHuggingFaceAsync(modelName, cancellationToken?, ...)");
Console.WriteLine("  â€¢ FromProvider(fileProvider, options?, progress?)");
Console.WriteLine("  â€¢ FromProviderAsync(fileProvider, cancellationToken?, ...)\n");

Console.WriteLine("ğŸ”¤ ENCODING:");
Console.WriteLine("  â€¢ Encode(text, addEos?) â†’ int[]");
Console.WriteLine("  â€¢ EncodeDetailed(text, addEos?) â†’ EncodingResult");
Console.WriteLine("  â€¢ EncodeBatch(texts[], addEos?) â†’ int[][]");
Console.WriteLine("  â€¢ CountTokens(text, addEos?) â†’ int\n");

Console.WriteLine("ğŸ”  DECODING:");
Console.WriteLine("  â€¢ Decode(ids[], skipSpecialTokens?) â†’ string");
Console.WriteLine("  â€¢ DecodeBatch(ids[][], skipSpecialTokens?) â†’ string[]\n");

Console.WriteLine("âš™ï¸  PROPERTIES:");
Console.WriteLine("  â€¢ VocabularySize â†’ int");
Console.WriteLine("  â€¢ Vocabulary â†’ IReadOnlyDictionary<string, int>");
Console.WriteLine("  â€¢ SpecialTokens â†’ IReadOnlyDictionary<string, int>");
Console.WriteLine("  â€¢ GetSpecialTokenId(tokenName) â†’ int?\n");

Console.WriteLine("ğŸ¤– ONNX:");
Console.WriteLine("  â€¢ PrepareForOnnx(text, maxLength?) â†’ (long[], long[])");
Console.WriteLine("  â€¢ PrepareForOnnxBatch(texts[], maxLength?) â†’ (long[,], long[,])\n");

Console.WriteLine("\n=== Sample Complete ===");
Console.WriteLine("All examples demonstrate the Qwen3 Tokenizer capabilities.");
Console.WriteLine("For more information, visit: https://github.com/yuniko-software/qwen3-tokenizer...\n");
